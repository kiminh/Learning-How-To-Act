{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counterfactual Estimation and Optimization of Click Metrics for Search Engines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem addressed by the paper\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper addresses the problem of an unbiased evaluation of online metric like CTR by using causal infernce \n",
    "techniques in a contextual multi-arm bandit framework. This approach supports to run potentially infinite number of A/B tests offline using search logs and there by optimizing search engines fastly and inexpensively \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Contextual-Bandit formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a k-arm bandit setup an agent(also known as learner) interacts with an enviorment with a goal of learning true reward distribution of arms, which are unknown to the learner. Bandits employ explore-exploit tradeoff to estimate the true unkown reward distribution of arms and it does this by minimizing the average regret it receives from the environment. In a contextual-bandit variant, reward distribution of an arm is both a function of context and the profile of an arm. Enviornment picks a context(x) and it's corresponding k-dimensional reward vector($\\vec{r}$) and it reveals context x to the learner. Upon observing x, learner picks an arm basis context and it's exploration-exploitation tradeoff strategy and inturn receives bandit feedback from the enviornment. One thing to note here is learner receives the feedback for the arm it picked and don't receive any feedback for the arms which it didn't pick. Received feedback is known as bandit feedback and non-received feedback is known as counterfactual outcomes. \n",
    "Because of the bandit feedback received by the learner, we can evaluate a new policy${\\pi}$ only for the cases where logged policy matches with new policy that's $\\pi(x) == a$. \n",
    "\n",
    "Contextual-bandits are measured through average reward they receive from the enviornment, which we call as the value of the policy\n",
    "\n",
    "$V(\\pi) := E_{(x,\\vec{r})}[ r_{\\pi(x)}] $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Approach\n",
    "Because of the nature of contextual-bandit setting, sampling bias might be introduced by the logging policy wherein propensity score distribution of arms varies from a context to context which might hinder unbiased offline evlauation of a new policy. This paper propose use of inverse propensity score to correct the sampling bias and be able to estimate unbiased offline evaluation of a new policy. Division by inverse propensity is the trick here\n",
    "\n",
    "$\\hat{V}_{offline}(\\pi)$ $= \\sum_ {(x,a,r_{a}, p_{a}) \\in D}$  $\\frac{ r_{a}I(\\pi(x) == a) }{ p_a} $\n",
    "\n",
    "where x is context, a is action, r_a is reward received, p_a is propensity of selecting arm a ( Note D comes from logged policy).\n",
    "\n",
    "The useful property is $E[ {\\hat{V}_{offline}}] = V(\\pi)$, which makes $\\hat{V}_{offline}$ an unbiased metric.\n",
    "\n",
    "As we can observe we want the logging policy should have non-zero propensity scores for arms select by a new policy for a given conext and moreover logging policy has zero knowledge of new polices that we are going to come up with in future. So one of the crude logging policy strategy can be try each arm by 1/k propensity score which might hamper business metrics and in general existing policies are already decently optimized. This paper proposed injecting randomization into the existing system and enforces minimum value on $p_{a}$ to have tight bounds on $\\hat{V}_{Offline}$confidence interval.\n",
    "\n",
    "Paper proposed interesting tricks to validate logged policy like harmonic mean property of propensity score & logging of seed(which reset pseudo random generation with context & timestamp).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation\n",
    "This paper is using proposed offline evaluation procedure to evaluate speller check which is a sub-routine in modern recommendation engine. As mentioned in approach section, a trick is proposed to inject randomization into existig policy with the following formula $P(q_{i}) = \\frac{1}{1+exp(\\lambda_{1}(s_{1}-s_{i}) + \\lambda_{2}}$\n",
    "where $\\lambda_{1} and \\lambda_{2}$  controls tradeoff of exploration & exploitation. Where $P(q_i)$ is probability of rewritten query is chosen in an action. \n",
    "\n",
    "With the above mentioned randomization they collected logged data and at the same time they ran a new policy on separate population. Authors compared online metric with offline metric of new policy on the logged data and they showed empirically unbiasedness of $\\hat{V}_{offline}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Things which are not clear to me in the paper\n",
    "Interestingly what's not clear to me in the paper is how authors formulated Speller checker problem into contextual multi-arm bandit setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
