{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinUCB():\n",
    "    '''\n",
    "        This class implements LinUCB Disjoint algorithm\n",
    "    '''\n",
    "    \n",
    "    def __init__( self,  arm_ids, alpha, ctxtVecLen):\n",
    "        \n",
    "        '''\n",
    "            alpha : Controls exploration; \n",
    "                    Higher the alpha, wider the confidence interval and higher the chance of trying any given arm;\n",
    "           \n",
    "            kArms: Number of arms\n",
    "            \n",
    "            ctxtVecLen: length of the feature vector\n",
    "            \n",
    "        '''\n",
    "        \n",
    "        self.arm_ids    = arm_ids\n",
    "        self.kArms      = len(arm_ids)\n",
    "        self.alpha      = alpha\n",
    "        self.ctxtVecLen = ctxtVecLen\n",
    "        \n",
    "        # A: (ctxtVecLen x ctxtVecLen) matrix = D_a.T * D_a + I_d  ( Where a is indexing on arms array)\n",
    "        # The inverse of A is used in ridge regression \n",
    "        self.A_a = {}\n",
    "        for idx, arm_id in enumerate(arm_ids):\n",
    "            self.A_a[arm_id] = np.identity(ctxtVecLen) # A = D_T * D ( Where D is n*p matrix)\n",
    "\n",
    "        # b: (ctxtVecLen x 1) corresponding response vector. \n",
    "        # Equals to D_a.T * c_a in ridge regression formulation\n",
    "        self.b_a = {}\n",
    "        for idx, arm_id in enumerate(arm_ids):\n",
    "            self.b_a[arm_id] = np.zeros([ctxtVecLen,1]) # b = D_T * RV ( where RV is n*1 matrix )\n",
    "        \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'LinUCB Object with {kArms} arms and with alpha {alpha}'.format(self.alpha, self.kArms, self.ctxtVecLen) \n",
    "    \n",
    "    def select_arm(self, ctxtVec, random_policy=False):\n",
    "        ''' Returns the index of the next arm to pull '''    \n",
    "        \n",
    "        # Reshape covariates input into (d x 1) shape vector\n",
    "        ctxtVec = ctxtVec.reshape([-1,1])\n",
    "\n",
    "        ucb = []\n",
    "        for idx, arm_id in enumerate(np.random.permutation(list(LinUCBPolicy.A_a.keys()) )):\n",
    "            # Find A inverse for ridge regression\n",
    "            A_inv = np.linalg.inv(self.A_a[arm_id])\n",
    "            \n",
    "            # Perform ridge regression to obtain estimate of covariate coefficients theta\n",
    "            # theta is (ctxtVecLen x 1) dimension vector\n",
    "            theta = np.dot(A_inv, self.b_a[arm_id])\n",
    "            \n",
    "            ucb.append( \n",
    "                (\n",
    "                    arm_id,\n",
    "                    (np.dot(theta.T, ctxtVec) + self.alpha * np.sqrt(np.dot(ctxtVec.T, np.dot(A_inv,ctxtVec))))[0][0] \n",
    "                )\n",
    "            )                \n",
    "        if random_policy == True:    \n",
    "            return np.random.choice(self.arm_ids)\n",
    "        else:\n",
    "            return( max(ucb,key=lambda x:x[1])[0] )            \n",
    "    \n",
    "    def update(self, ctxtVec, chosen_arm, reward):\n",
    "        '''        \n",
    "            After we pull an arm, we get a reward signal back from our system. This function update our algorithm's beliefs\n",
    "            about the quality of the arm we just chosen by providing this reward information.\n",
    "        \n",
    "            chosen_arm : The numeric index of the most recently chosen arm\n",
    "            reward     : The reward received from chossing that arm\n",
    "        '''\n",
    "        \n",
    "        # Reshape covariates input into (d x 1) shape vector\n",
    "        ctxtVec = ctxtVec.reshape([-1,1])        \n",
    "    \n",
    "        # Update A which is (d * d) matrix.\n",
    "        self.A_a[chosen_arm] += np.dot(ctxtVec, ctxtVec.T)\n",
    "        \n",
    "        # Update b which is (d x 1) vector\n",
    "        # reward is scalar\n",
    "        self.b_a[chosen_arm] += reward * ctxtVec                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Estimated Reward:\t 0.1117948717948718\n",
      "armSelCntr:  Counter({9: 107, 1: 106, 7: 103, 6: 103, 8: 103, 4: 99, 5: 95, 10: 94, 3: 87, 2: 78})\n",
      "armRewCntr:  Counter({7: 25.0, 2: 23.0, 9: 18.0, 3: 15.0, 6: 9.0, 4: 6.0, 8: 6.0, 10: 5.0, 5: 2.0, 1: 0.0})\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Policy\n",
    "ctxtVecLen = 100\n",
    "arms_ids = list( range(1, kArms+1))\n",
    "LinUCBPolicy = LinUCB(arms_ids, alpha, ctxtVecLen)        \n",
    "\n",
    "armSelCntr = Counter()\n",
    "armRewCntr = Counter()\n",
    "\n",
    "random_policy = True\n",
    "with open('./dataset.txt') as fp:\n",
    "        for idx, eachRecord in enumerate(fp):\n",
    "            data_arm = int(eachRecord.split(' ')[0])\n",
    "            data_reward = float(eachRecord.split()[1])\n",
    "            covariate_string_list = eachRecord.split()[2:]        \n",
    "            data_x_array = np.array([float(eCov) for eCov in eachRecord.split()[2:]])\n",
    "            selected_arm = LinUCBPolicy.select_arm( data_x_array, random_policy )\n",
    "\n",
    "            if selected_arm == data_arm:\n",
    "                armSelCntr[selected_arm] += 1\n",
    "                LinUCBPolicy.update(data_x_array, selected_arm, data_reward) \n",
    "                armRewCntr[selected_arm] += data_reward\n",
    "\n",
    "print('Total Estimated Reward:\\t', sum(armRewCntr.values())/sum(armSelCntr.values()) )\n",
    "print('armSelCntr: ', armSelCntr)\n",
    "print('armRewCntr: ', armRewCntr)\n",
    "print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinUCB Object with 10 arms and with alpha 0\n",
      "Total Estimated Reward:\t 0.7588294651866802\n",
      "armSelCntr:  Counter({2: 309, 7: 210, 3: 207, 9: 130, 6: 77, 4: 23, 5: 10, 1: 9, 10: 8, 8: 8})\n",
      "armRewCntr:  Counter({2: 262.0, 7: 180.0, 3: 134.0, 9: 107.0, 6: 58.0, 4: 11.0, 10: 0.0, 8: 0.0, 5: 0.0, 1: 0.0})\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LinUCB Object with 10 arms and with alpha 0.25\n",
      "Total Estimated Reward:\t 0.4070707070707071\n",
      "armSelCntr:  Counter({2: 175, 7: 121, 3: 105, 9: 104, 6: 86, 4: 83, 8: 81, 5: 80, 10: 80, 1: 75})\n",
      "armRewCntr:  Counter({2: 139.0, 7: 80.0, 9: 61.0, 3: 57.0, 6: 26.0, 4: 14.0, 5: 11.0, 8: 8.0, 1: 4.0, 10: 3.0})\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LinUCB Object with 10 arms and with alpha 0.5\n",
      "Total Estimated Reward:\t 0.27353815659068387\n",
      "armSelCntr:  Counter({2: 148, 7: 107, 9: 106, 3: 103, 6: 93, 4: 92, 8: 91, 1: 90, 5: 90, 10: 89})\n",
      "armRewCntr:  Counter({2: 96.0, 7: 49.0, 9: 41.0, 3: 40.0, 6: 18.0, 5: 11.0, 4: 10.0, 8: 5.0, 10: 4.0, 1: 2.0})\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LinUCB Object with 10 arms and with alpha 0.75\n",
      "Total Estimated Reward:\t 0.21313131313131314\n",
      "armSelCntr:  Counter({2: 127, 7: 105, 9: 100, 3: 97, 6: 96, 10: 95, 4: 94, 5: 93, 8: 92, 1: 91})\n",
      "armRewCntr:  Counter({2: 69.0, 7: 43.0, 9: 32.0, 3: 25.0, 6: 16.0, 4: 8.0, 8: 5.0, 5: 5.0, 10: 5.0, 1: 3.0})\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LinUCB Object with 10 arms and with alpha 1.0\n",
      "Total Estimated Reward:\t 0.18467583497053044\n",
      "armSelCntr:  Counter({2: 127, 7: 107, 9: 103, 8: 99, 3: 99, 6: 98, 1: 97, 10: 96, 4: 96, 5: 96})\n",
      "armRewCntr:  Counter({2: 63.0, 7: 36.0, 9: 30.0, 3: 20.0, 6: 16.0, 4: 9.0, 5: 5.0, 1: 4.0, 8: 3.0, 10: 2.0})\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LinUCB Object with 10 arms and with alpha 1.25\n",
      "Total Estimated Reward:\t 0.16981132075471697\n",
      "armSelCntr:  Counter({2: 121, 7: 103, 3: 102, 4: 100, 9: 99, 6: 98, 8: 96, 1: 96, 10: 96, 5: 96})\n",
      "armRewCntr:  Counter({2: 58.0, 7: 32.0, 3: 23.0, 9: 22.0, 6: 12.0, 4: 7.0, 5: 6.0, 8: 4.0, 10: 4.0, 1: 3.0})\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LinUCB Object with 10 arms and with alpha 1.5\n",
      "Total Estimated Reward:\t 0.14404145077720207\n",
      "armSelCntr:  Counter({2: 112, 5: 97, 6: 97, 8: 96, 7: 95, 10: 95, 3: 94, 9: 94, 1: 93, 4: 92})\n",
      "armRewCntr:  Counter({2: 48.0, 7: 26.0, 3: 18.0, 9: 15.0, 6: 13.0, 4: 6.0, 8: 5.0, 5: 4.0, 1: 2.0, 10: 2.0})\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_policy = False\n",
    "estimated_rewards_alphas = {}\n",
    "for alpha in [0, 0.25, 0.5, 0.75, 1.0, 1.25, 1.5]:\n",
    "#     alpha      = 0.25  # scalar factor on confidence interval ( higher the value higher the exploration )\n",
    "    ctxtVecLen = 100\n",
    "    arms_ids = list( range(1, kArms+1))\n",
    "    LinUCBPolicy = LinUCB(arms_ids, alpha, ctxtVecLen)        \n",
    "    print(LinUCBPolicy)\n",
    "\n",
    "    armSelCntr = Counter()\n",
    "    armRewCntr = Counter()\n",
    "\n",
    "    with open('./dataset.txt') as fp:\n",
    "        for idx, eachRecord in enumerate(fp):\n",
    "            data_arm = int(eachRecord.split(' ')[0])\n",
    "            data_reward = float(eachRecord.split()[1])\n",
    "            covariate_string_list = eachRecord.split()[2:]        \n",
    "            data_x_array = np.array([float(eCov) for eCov in eachRecord.split()[2:]])\n",
    "            selected_arm = LinUCBPolicy.select_arm( data_x_array, random_policy )\n",
    "\n",
    "            if selected_arm == data_arm:\n",
    "                armSelCntr[selected_arm] += 1\n",
    "                LinUCBPolicy.update(data_x_array, selected_arm, data_reward) \n",
    "                armRewCntr[selected_arm] += data_reward\n",
    "\n",
    "    print('Total Estimated Reward:\\t', sum(armRewCntr.values())/sum(armSelCntr.values()) )\n",
    "    print('armSelCntr: ', armSelCntr)\n",
    "    print('armRewCntr: ', armRewCntr)\n",
    "    print('\\n\\n\\n')\n",
    "    \n",
    "    estimated_rewards_alphas[alpha] = sum(armRewCntr.values())/sum(armSelCntr.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
